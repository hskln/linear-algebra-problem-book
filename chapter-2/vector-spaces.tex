\section{Vector spaces}

Real numbers can be added, and so can pairs of real numbers. If $\R^2$ is the set of all ordered pairs $(\alpha, \beta)$ of real numbers, then it is natural to define the sum of two elements of $\R^2$ by writing

\begin{equation}
    (\alpha,\beta) + (\gamma, \delta) = (\alpha+\gamma, \beta+\delta)
\end{equation}

and the result is that $\R^2$ becomes an abelian group. There is also a kind of partial multiplication that makes sense and is useful, namely the process of multiplying an element of $\R^2$ by a real number and thus getting another element of $\R^2$:

\begin{equation}
    \alpha(\beta, \gamma) = (\alpha\beta, \alpha\gamma)
\end{equation}

The end result of these comments is a structure consisting of three parts: an abelian group, namely $\R^2$, a field, namely $\R$, and a way of multiplying the elements of the group by the elements of the field.

For another example of the kind of structure that linear algebra studies, consider the set $\P$ if all polynomials with real coefficients. The set $\P$, endowed with the usual notion of addition of polynomials, is an abelian group. Just as in the case of $\R^2$ there is a multiplication that makes useful sense, namely the process of multiplying a polynomial $p$ by a real number:

\begin{equation}
    (\alpha p)(x) = \alpha \cdot p(x)
\end{equation}

The result is, as before, a triple structure: an abelian group $\P$, a field $\R$, and a way of multiplying elements of $\P$ by elements of $\R$.

The modification of replacing the set $\P$ of all real polynomials (\textbf{real polynomial} is a handy abbreviation for "polynomials with real coefficients")by the set $\P_3$ if all real polynomials of degree less than or equal to 3 is sometimes more natural to use than the unmodified version. The sum of two elements of $\P_3$ is again an element of $\P_3$ and so is the product of an element of $\P_3$ by a real number, and that's all there is to it.

One more example, and that will be enough for now. This time let $\V$ be the set of all ordered triples $(\alpha,\beta, \gamma)$ of real numbers such that

\begin{equation}
    \alpha + \beta + \gamma = 0
\end{equation}

Define addition by

\begin{equation}
    (\alpha, \beta, \gamma) + (\alpha', \beta', \gamma') = (\alpha + \alpha', \beta + \beta', \gamma + \gamma')
\end{equation},

define multiplication by

\begin{equation}
    \alpha(\beta, \gamma, \delta) = (\alpha\beta, \alpha\gamma, \alpha\delta)
\end{equation}

and observer that the result is always an ordered triple with sum zero. The set of all such triples is, once again, an abelian group (namely $\V$), a field, and a sensible way of multiplying group elements by field elements.

The general concept of a vector space is an abstraction of examples such as the ones just seen; it is a triple consisting of an abelian group, a field, and a multiplication between them. Recall, however, that it is immortal, illegal, and unprofitable to endow a set with two or more mathematical structures without tightly connecting them, s that each of them is restricted in an essential way. (The best known instance where that somewhat vague commandment is religiously obeyed is the definition of a field in Problem 16; there is an addition, there is a multiplication, and there is the essential connection betwene them, namely the distributive law.)

A \textbf{vector space over a field} $\F$ (of elements called \textbf{scalars}) is an additive (commutative) group $\V$ (of elements called \textbf{vectors}), together with an peration that assigns to each scalar $\alpha$ and each vector $x$ a product $\alpha x$ that is again a vector. For such a definition to make good mathematical sense, the operation (called \textbf{scalar multiplication}) should be suitably related to the three given operations (addition in $\F$, addition in $\V$, and multiplication in $\F$). The conditions that present themselves most naturally are these.

The vector distributive law:

\begin{equation}
    (\alpha + \beta) x = \alpha x + \beta x
\end{equation}

whenever $\alpha$ and $\beta$ are scalars and $x$ is a vector. (In other words, multiplication by a vector distributes over scalar addition.)

The scalar distributive law

\begin{equation}
    \alpha(x+y) = \alpha x + \alpha y
\end{equation}

whenever $\alpha$ is a scalar and $x$ and $y$ are vectors. (In other words, multiplication by a scalar distributes over vector addition).

The associative law

\begin{equation}
    (\alpha\beta)x = \alpha (\beta x)
\end{equation}

whenever $\alpha$ and $\beta$ are scalars and $x$ is a vector.

The scalar identity law

\begin{equation}
    1x = x
\end{equation}

for every vector $x$. (In other words, the scalar 1 acts as the identity transformation on vectors.)

(The reader has no doubt noticed that in scalar multiplication the scalar is always on the left and the vector on the right -- since the other kind of multiplication is not even defined, it makes no sense to speak of a commutative law. Nothing is lost by this convention, and something is gained: the very symbol for a product indicates which factor is the scalar and which the vector.)

Many questions can and should be asked about the conditions that define vector spaces: one worrisome question has to do with multiplication, and another one, easier, has to do with zero.

Why, it is natural to ask, is a multiplicative structure not imposed on vector spaces? Wouldn't it be natural and useful to define $(\alpha, \beta) \cdot (\gamma, \delta) = (\alpha\gamma, \beta\delta)$ (similarly to how addition is defined in $\R^2$)? The answer is no. The trouble is that even after the zero element (that is, the element (0,0)) of $\R^2$ is discarded, the remainder does not constitute a group; a pair that has one of its coordinates equal to 0, such, for instance, as (1,0), does not have an inverse. The same question for $\P$ is tempting: the element of $\P$ can be multiplied as well as added. Once again, however, the result does not convert $\P$ into a field; the multiplicative inverse of a polynomial is very unlikely to be a polynomial. Examples such as $\P_3$ add to the discouragement: the product of two elements of $\P_3$ might not be an element of $\P_3$ (the degree might be too large). The example of triples with sum zero is perhaps the most discouraging: the attempt to define the product of two elements of $\V$ collapses almost before it is begun. Even if both $\alpha + \beta + \gamma = 0$ and $\alpha' + \beta' + \gamma' = 0$, it is a rare coincidence that $\alpha\alpha' + \beta\beta' + \gamma\gamma' = 0$. It is best, at this stage, to resist the temptation to endow vector spaces with a multiplication.

Both the scalar 0 and the vector 0 have to do with addition; how do they behave with respect to multiplication? It is possible that they misbehave, in a sense smething like the one for vector multiplication discussed in the preceeding paragraph, and it is possible that they are perfectly well behaved- which is true?

\begin{problem}
Do the \textbf{scalar zero law}

\begin{equation}
    0x = 0
\end{equation}

and the \textbf{vector zero law}

\begin{equation}
    \alpha0 = 0
\end{equation}

follow from the conditions in the definition of vector spaces, or could they be false?
\end{problem}

\textit{Solution.} The scalar zero is a consequence of the other conditons; here is how the simple proof goes. If $x$ is in $\V$, then

\begin{align}
    0x + 0x & = (0+0)x \text{ (by the vector distributive law)} \\
            & = 0x
\end{align}

and therefore, simply by cancellation in the additive group $\V$, the forced conclusion is that $0x=0$.

As for the vector zero law, the scalar distributive law implies that $\alpha 0$ is always zero. Indeed:

\begin{equation}
    \alpha 0 + \alpha 0 = \alpha(0+0) = \alpha 0
\end{equation}

and therefore, simply by cancellation in the additive group $\V$, the forced conclusion is that $\alpha 0 = 0$.

It is good to know that these two results about 0 are in a sense best possible. That is: if $\alpha x = 0$, then either $\alpha = 0$, or $x = 0$. Reason: if $\alpha x = 0$, and $\alpha \neq 0$, then

\begin{equation}
    x = 1x = (\frac{1}{\alpha} \alpha)x = (\frac{1}{\alpha}) (\alpha x) \text{ (by the associative law)},
\end{equation}

which implies that

\begin{equation}
    x = (\frac{1}{\alpha}) 0 = 0
\end{equation}

\textit{Comment.} If a scalar multiplication satisfies all the conditions in the definition of a vector space, how likely is it that $\alpha x = x$? That happens when $x = 0$ and it happens when $\alpha = 1$; can it happen any other way? The answer is no; and, by now, the proof is easy; if $\alpha x = x$, then $(\alpha - 1)x = 0$, and therefore either $\alpha - 1 = 0$ or $x = 0$.

A pertinent comment is that every field is a vector space over itself. Isn't that obvious? All it says is that given $\F$, and if the space $\V$ is defined to be $\F$ itself, with addition in $\V$ being what it was in $\F$, and scalar multiplication being ordinary multiplication in $\F$, then the conditions in the definition of a vector space are automatically satisfied. Consequence: if $\F$ is a field, then the equation $0x=0$ in $\F$ is an instance of the scalar zero law. In other words, the solution of Problem 17 is a special case of the present one.

\section{Examples}

It is always important, in studying a mathematical structure, to be able to recognize an example as a proper one, and to recognize a pretender as one that fails in some respect. Here are a half dozen candidates that may be vector spaces or may be pretenders.

\begin{enumerate}
    \item Let $\F$ be $\C$, and let $\V$ also be the set $\C$ of complex numbers. Define addition in $\C$ the usual way, and let scalar multiplication (denoted by $\times$) be defined as follows:

          \begin{equation}
              \alpha \ast x = \alpha^2 \cdot x
          \end{equation}

    \item Let $\F$ be a field, let $\V$ be $\F^2$ (the set of all ordered pairs of elements of $\F$), let addition in $\V$ be the usual one (coordinatewise), and define a new scalar multiplication by writing

          \begin{equation}
              \alpha \ast (\beta, \gamma) = (\alpha\beta, 0)
          \end{equation}

          (for all $\alpha,\beta,\gamma$)

    \item Let $\F$ be the field of four elements discussed in Problem 19, let $\V$ be $\F^2$ with the usual addition, and define scalar multiplication by

          \begin{equation}
              \alpha \ast (\beta, \gamma) = (\alpha\beta, \alpha\gamma) \text{ if } \gamma \neq 0
          \end{equation}

          and

          \begin{equation}
              \alpha \ast (\beta,\gamma) = (\alpha^2 \beta, 0)
          \end{equation}

    \item Let $\F$ be $\R$ and let $\V$ be the set $\R_+$ of all positive real numbers. Define the "sum" denoted by $\alpha \boxedplus \beta$ of any two positive real numbers $\alpha$ and $\beta$, and define the "scalar product" denoted by $\alpha \boxeddot \beta$ of any positive real number $\alpha$ by an arbitrary (not necessarily positive) real number $\beta$ as follows:

          \begin{equation}
              \alpha \boxedplus \beta = \alpha\beta
          \end{equation}

          and

          \begin{equation}
              \alpha \boxeddot \beta = \beta^\alpha
          \end{equation}

    \item Let $\F$ be $\C$, and let $\V$ be $\C$ also. Vector addition is to be defined in the ordinary addition of complex numbers, but the product of a scalar $\alpha$ (in $\C$) and a vector $x$ (in $\C$) is to be defined by forming the real part of $\alpha$ first. That is:

          \begin{equation}
              \alpha \cdot x = (\operatorname{\Re} \alpha)x
          \end{equation}

    \item Let $\F$ be the field $\Q$ of rational numbers, let $\V$ be the fields $\R$ of real numbers and define scalar multiplication by writing

          \begin{equation}
              \alpha \ast x = \alpha x
          \end{equation}

          for all $\alpha$ in $\Q$ and all $x$ in $\R$.
\end{enumerate}

\begin{problem}
Which of the defining properties of vector spaces are missing in the examples 1, 2, 3, 4, 5 and 6?
\end{problem}

\textit{Solution. }

(1) The scalar distributive law fails; indeed

\begin{equation}
    2 \ast 1 = 2^2 \cdot 1 = 4,
\end{equation}

but

\begin{equation}
    1 \ast 1 + 1 \ast 1 = 1 \ast 1 + 1 \ast 1 = 2.
\end{equation}

The verifications that all the other axioms of a vector space are satisfied are painless routine.

(2) The scalar identity law fails; all the other conditions are satisfied.

(3) Since the mapping $\alpha \mapsto \alpha^2$ is multiplicative ($(\alpha\beta)^2 = \alpha^2 \beta^2$), the associative law for the new scalar product is true (this should be checked, and it is fun to check). The new scalar identity law follows from the fact that $1^2 = 1$. The verification of the new scalar distributive law depends on the fact that if $\alpha$ and $\beta$ are scalars ( in the present sense, a very special case), then

\begin{equation}
    (\alpha + \beta)^2 =  \alpha^2 + \beta^2,
\end{equation}

(That identity holds, in fact, if and only if the field has "characteristic 2", which means that $\alpha + \alpha = 0$ for every $\alpha$ in $\F$. An equivalent way of expressing that condition is just to say that $2 = 0$, where "$2$" means $1+1$, of course.) The scalar distributive law, however, is false. Indeed:

\begin{equation}
    1((1,0) + (0,1)) = 1(1,1) = (1,1),
\end{equation}

whereas

\begin{equation}
    1(1,0) + 1(0,1) = (1+1,0) + (0,1) = (1+1,1).
\end{equation}

(4) Nothing is missing; the definition of $\boxedplus$ and $\boxed{\cdot}$ do indeed make $\R$, into a real vector space.

(5) In this example the associative law fails. Indeed, if $\alpha = \beta = i$, then

\begin{equation}
    (\alpha\beta)\cdot 1 = (-1)1 = -1,
\end{equation}

whereas

\begin{equation}
    \alpha \cdot (\beta \cdot 1) = 0 \cdot (0) = 0.
\end{equation}

The verifications of the distributive laws (vector or scalar), and of the scalar identity law, are completely straightforward; all that they depend on (in addition to the elementary properites of the addition of complex numbers) is that \textit{Re} does the right thing with 0, 1, and $+$. (The right thing is $\textit{Re}0 = 0$, $\textit{Re}1=1$, and $\textit{Re}(\alpha+\beta) = \textit{Re} \alpha + \textit{Re}\beta$).

(6) Here, once more, nothing is missing. The result is a special case of the general observation that if $\F$ is a field and $\mathbf{G}$ is a subfield, then $\F$ is a vector space over $\mathbf{G}$.

\textit{Question.} What is the status of the zero laws (scalar and vector) in these examples? The proof that they held (Problem 20) depended on the truth of the other conditions; does the failure of some of those conditons make the zero laws fail also?