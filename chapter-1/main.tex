\chapter{SCALARS}

\newtheorem{problem}{Problem}
\section{Double Addition}

Is it obvious that

\begin{equation}
    63 + 48 = 27 + 84?
\end{equation}

It is a true and uninteresting mathematical statement that can be verified in a few seconds - but is it obvious? If calling it obvious means that the reason for its truth is clearly understood, without even a single second's verification, then most people would probably say no.

What about

\begin{equation}
    (27+30)+48=27+(30+48)
\end{equation}

- is that obvious? Yes it is, for most people; the instinctive (and correct) reaction is that the way the terms of a sum are bunched together cannot affect the answer. The approved technical term is not "bunch together" but "associative", the instinctive reaction is a readiness to accept what is called the \textbf{associative law} of addition for real numbers. (Surely every reader has noticed by now that the non-obvious statement and the obvious one are in some sense the same:

\begin{equation}
    63 = 27 + 36 \text{ and } 84 = 36 + 48
\end{equation})

Linear algebra is concerned with several different kinds of operations (such as addition) on several different kinds of objects (not necessarily real numbers). To prepare the ground for the study of strange operations and to keep the associative law from being unjustly dismissed as a triviality, a little effort to consider some good examples and some bad ones is worthwhile. Some of the examples will be useful in the sequel, and some won't - some are here to show that associativity can fail, and others are here to show that even when it holds it may be far from obvious. In the world of linear algebra non-associative operations are rare, but associative operations whose good behaviour is not obvious are more frequently met.


\begin{problem}
If a new addition for real numbers, denoted by the temporary symbol $\boxed{+}$ is defined by

\begin{equation}
    \alpha \boxed{+} \beta = 2\alpha + 2\beta
\end{equation}

is $\boxed{+}$ associative?
\end{problem}

\textit{Comment.} The plus sign on the right-hand side of the equation denotes ordinary addition.

Note: since ordinary addition is commutative, so that

\begin{equation}
    2\alpha + 2\beta = 2\beta + 2\alpha
\end{equation}

it follows that

\begin{equation}
    \alpha \boxed{+} \beta = \beta \boxed{+} \alpha
\end{equation}

Conclusion: the new addition is also commutative.

\section{Half double addition}

\begin{problem}
If a new addition of real numbers, denoted by the temporary symbol $\boxed{+}$, is defined by

\begin{equation}
    \alpha \boxed{+} \beta = 2\alpha + \beta
\end{equation}

is $\boxed{+}$ associative?
\end{problem}

\textit{Comment.} Since $2\alpha + \beta$ is usually different from $2\beta + \alpha$, this $\boxed{+}$ is not commutative.

\textit{Additional comment.} It is notable to comment that this operation is not commutative - after all, the primary goal of this book is to talk about linear algebra, and linear transformation is usually not commutative. Transformations are functions, and thus are not usually commutative.

To the matter at hand, is this operation associative? We examine the results of $(\alpha \boxed{+} \beta) \boxed{+} \gamma$ and $\alpha \boxed{+} (\beta \boxed{+} \gamma)$.

\begin{align}
    (\alpha \boxed{+} \beta) \boxed{+} \gamma & = (2\alpha + \beta) \boxed{+} \gamma \\
                                              & = 2(2\alpha + \beta) + \gamma        \\
    \alpha \boxed{+} (\beta \boxed{+} \gamma) & = \alpha \boxed{+} (2\beta + \gamma) \\
                                              & = 2\alpha + 2\beta + \gamma
\end{align}

From here it is easy to see that the operation is not associative. Was there a more elegant way we could have reasoned about the process?

\section{Exponentiation}

\begin{problem}
If an operation for positive integers, denoted by the temporary symbol $\ast$, is defined by

\begin{equation}
    \alpha \ast \beta = \alpha^\beta
\end{equation}

is it commutative? Is it associative?
\end{problem}

\textit{Additional comment.} The exponentiation operation is not commutative, since $\alpha^\beta$ is usually different from $\beta^\alpha$. It is also not associative, although this fact is not as obvious:

\begin{align}
    (\alpha \ast \beta) \ast \gamma & = (\alpha^\beta)\ast\gamma   \\
                                    & = \alpha^{\beta\gamma}       \\
    \alpha \ast (\beta \ast \gamma) & = \alpha \ast (\beta^\gamma) \\
                                    & = \alpha^{\beta^\gamma}
\end{align}

Since usually $\beta\gamma$ is not the same as $\beta^\gamma$, these two expressions are different.

\section{Complex numbers}

Suppose that an operation $\boxed{+}$ is defined for ordered pairs of real numbers, that is for objects that look like $(\alpha, \beta)$ with both $\alpha$ and $\beta$ real, as follows:

\begin{equation}
    (\alpha, \beta) \boxed{+} (\gamma, \delta) = (\alpha + \beta, \gamma + \delta)
\end{equation}

Is it commutative? Sure, obviously - how could it miss? All it does is perform the known commutative operation of addition of real numbers twice, once for each of the coordinates. Is it associative? Sure, obviously, for the same reason.

The double addition operation in Problem 1 and 2 are artificial; they were cooked up to make a point. The operation of exponentiation in Problem 3 is natural enough, and that is its point: "natural" operations can fail to be associative. The coordinatewise addition here defined for ordered pairs is a natural one also, but it is far from the only one that is useful.

\begin{problem}
If an operation for ordered pairs of real numbers, denoted by the temporary symbol $\boxed{\cdot}$, is defined by

\begin{equation}
    (\alpha, \beta) \boxed{\cdot} (\gamma, \delta) = (\alpha\gamma - \beta\delta, \alpha\delta + \beta\gamma)
\end{equation}

is it commutative? Is it associative?
\end{problem}

\textit{Comment.} The reason for the use of the symbol $\boxed{\cdot}$ (instead of $\boxed{+}$) is twofold; it is reminiscent of multiplication (instead of addition), and it avoids confusion when the two operations are discussed simultaneously (as in many contexts they must be).

\textit{Additional comment.} This is, of course, the multiplication operation for complex numbers. If we were to alternatively define a number $i$ such that $i^2 = -1$, then this operation becomes a combination of multiplication and addition of real numbers which we have known to be associative and commutative. This operation, by that reason, is also associative and commutative.

\section{Affine transformation}

Looking strange is not necessarily a sign of being artificial or useless.

\begin{problem}
If an operation for ordered pairs of real numbers, denoted by $\boxed{\cdot}$ again, is defined by

\begin{equation}
    (\alpha, \beta) \boxed{\cdot} (\gamma, \delta) = (\alpha\gamma, \alpha\delta + \beta)
\end{equation}

is it commutative? Is it associative?
\end{problem}

\textit{Additional comment.} An Affine transformation is a Euclidean transformation that preserves lines and parallelism, but not necessarily angls and distances.

Our intuition from geometry is that transformations are not necessarily commutative (transforming a figure one way and then another may not necessarily result in the same figure had we done it the other way), but may be associative (as long as the same set and relative order of transformations are taking place, it does not matter which one we carry out first). We try to verify this algebraically:

\begin{align}
    (\gamma, \delta) \boxed{\cdot} (\alpha, \beta) & = (\gamma\alpha, \gamma\beta + \delta)
\end{align}

As we suspected, the transformation is not commutative.

Verifying associativity is a lot more tedious:

\begin{align}
    ((\alpha, \beta) \boxed{\cdot} (\gamma, \delta)) \boxed{\cdot} (\epsilon, \zeta)
     & = (\alpha\gamma, \alpha\delta + \beta) \boxed{\cdot} (\epsilon, \zeta) \\
     & = (\alpha\gamma\epsilon, \alpha\gamma\zeta + \alpha\delta + \beta)     \\
    (\alpha,\beta) \boxed{\cdot} ((\gamma,\delta) \boxed{\cdot} (\epsilon, \zeta))
     & = (\alpha, \beta) \boxed{\cdot} (\gamma\epsilon, \gamma\zeta + \delta) \\
     & = (\alpha\gamma\epsilon, \alpha(\gamma\zeta + \delta) + \beta)         \\
     & = (\alpha\gamma\epsilon, \alpha\gamma\zeta + \alpha\delta + \beta)
\end{align}

But it does show that our intuition was correct.

\section{Matrix multiplication}

The strange multiplication of Problem 5 is a special case of one that is more complicated but less strange.

\begin{problem}
If an operation for ordered quadruples of real numbers, denoted by $\boxed{\cdot}$, is defined by

\begin{equation}
    (\alpha, \beta, \gamma, \delta) \boxed{\cdot} (\alpha', \beta', \gamma', \delta') \\
    = (\alpha\alpha' + \beta\gamma', \alpha\beta' + \beta\delta', \gamma\alpha' + \delta\gamma', \gamma\beta + \delta\delta')
\end{equation}

is it commutative? Is it associative?
\end{problem}

\textit{Comment.} How is the multiplication of Problem 5 for ordered pairs of "special case" of this one? Easy: restrict attention to only those quadruples $(\alpha, \beta, \gamma, \delta)$ for which $\gamma = 0$ and $\delta = 1$. The $\boxed{\cdot}$ product of two such special quadruples is again such a special one; indeed if $\gamma = \gamma' = 0$ and $\delta = \delta' = 1$, then $\gamma\alpha' + \delta\gamma' = 0$ and $\gamma\beta' + \delta\delta' = 1$. The first two coordinates of the product are $\alpha\alpha'$ and $\alpha\beta' + \beta$, and that's in harmony with Problem 5.

Another comment may come as an additional pleasant surprise: the multiplication of complex numbers discussed in Problem 4 is also a special case of the quadruple multiplication discussed here. Indeed: restrict attention to only those quadruples that are of the form

\begin{equation}
    (\alpha, \beta, -\beta, \alpha)
\end{equation}

and note that

\begin{equation}
    (\alpha, \beta, -\beta, \alpha) \boxed{\cdot} (\gamma, \delta, -\delta, \gamma)                                      \\
    = (\alpha\gamma - \beta\delta, \alpha\delta + \beta\gamma, -\beta\gamma - \alpha\delta, -\beta\delta + \alpha\gamma)
\end{equation}

- in harmony with Problem 4.


\section{Modular multiplication}

Define an operation, denoted by $\boxed{\cdot}$, for the numbers $01,2,3,4,5$ as follows: multiply as usual and then throw away multiples of 6. (The technical expression is "multipy modulo 6".) Example: $4 \boxed{\cdot} 5 = 2$ and $2 \boxed{\cdot} 3 = 0$.

\begin{problem}
is multiplication modulo 6 commutative? Is it associative? What if 6 is replaced by 7: do the conclusions for 6 remain true or do they change?
\end{problem}

\textit{Additional comment.} This is a well known property of modulo operations, that they are an equivalence relation of normal arithmetic. Operations modulo n are associative and commutative.

\section{Small operations}

Problem 7 shows that interesting operations can exist on small sets. Small sets have the added advantage that sometimes they can forewarn us about some dangers that become more complicated, and therefore harder to see, when the sets get larger. Another reason small sets are good is that operations on them can be defined in a tabular manner that is reassuringly explicit.

Consider, for instance, the table:

\begin{center}
    \begin{tabular}{l|ccc}
        $\times$ & 0 & 1 & 2 \\
        \hline
        0        & 0 & 0 & 0 \\
        1        & 0 & 1 & 2 \\
        2        & 0 & 2 & 1
    \end{tabular}
\end{center}

which defines multiplication modulo 3 for the numbers $0,1,2$. The information such tables are intended to communicate is taht the product of the element at the left a \textbf{row} by the element at the top of a \textbf{column}, in that order, is the element placed where that row and that column meet. Example: $2 \times 2 = 1 \mod 3$.

It might be worth remarking that there is also a useful concept of addition modulo 3; it is defined by the table

\begin{center}
    \begin{tabular}{l|ccc}
        + & 0 & 1 & 2 \\
        \hline
        0 & 0 & 1 & 2 \\
        1 & 1 & 2 & 0 \\
        2 & 2 & 0 & 1
    \end{tabular}
\end{center}

It's a remarkable fact that addition and multiplication modulo 3 possess all the usually taught properties of the arithmetic operations bearing the same names. They are, for instance, both commutative and associative, they conspire to satisfy the distributive law

\begin{equation}
    \alpha \times (\beta + \gamma) = (\alpha \times \beta) + (\alpha \times \gamma)
\end{equation}

they permit unrestricted subtraction (so that, for example, $1 - 2 = 2$), and they permit division restricted only by the exclusion of the denominator 0 (so that, for example, $\frac{1}{2} = 2$). In a word (officially to be introduced and studied later) the integers modulo 3 form a \textbf{field}.

Problem 1 is about an operation that is commutative but not associative. Can that phenomenon occur in small sets?

\begin{problem}
Is there an operation in a set of three elements that is commutative but not associative?
\end{problem}

\textit{Additional comment.} There is no standard operation which meets the requirement.

\section{Identity elements}

The commonly accepted attitudes toward the commutative law and the associative law are different. Many real life operations fail to commute; the mathematical community has learned to live with that fact and even to enjoy it. Violations of the associative law, on the other hand, are usually considered by specialists only. Having made the point that the associative law deserves respect, this book will concentrate in the sequel on associative operations only. The next job is to see what other laudable properties such operations can and should possess.

The sum of $0$ and any real number $\alpha$ is $\alpha$ again; the product of $1$ and any real number $\alpha$ is $\alpha$ again. The phenomenon is described by saying that 0 and 1 are \textbf{identity elements} (or zero elements, or unit elements, or neutral elements) for addition and multiplication respectively. An operation that has an identity element is better to work with than one that doesn't. Which ones do?

\begin{problem}
Which of the operations
\begin{enumerate}
    \item double addition,
    \item half double addition,
    \item exponentiation,
    \item complex multiplication,
    \item multiplication of affine transformations,
    \item matrix multiplication,
    \item modular addition and multiplication
\end{enumerate}
have an identity element?
\end{problem}

\textit{Additional comment.}

\begin{enumerate}
    \item Double addition

          The made-up operation to demonstrate a point does not really have an identity element, as usually $\alpha$ is different from $2\alpha$.

    \item Half double addition

          Same as above.

    \item Exponentiation

          The identity element is 1. $\alpha^1 = \alpha$.

    \item Complex multiplication

          One must be a little more careful for this particular case. Suppose an identity element exist and let it be denoted by $(I_a, I_b)$.

          Then

          \begin{align}
              (\alpha, \beta) \boxed{\cdot} (I_a, I_b) & = (\alpha I_a - \beta I_b, \alpha I_b + \beta I_a)
          \end{align}

          Which implies

          \begin{align}
              \alpha & = \alpha I_a - \beta I_b \\
              \beta  & = \alpha I_b + \beta I_a
          \end{align}

          giving us the element $(1,0)$ as the identity.

    \item Multiplication of Affine transformation

          Recall the Affine transformation

          \begin{equation}
              (\alpha, \beta) \boxed{\cdot} (\gamma, \delta) = (\alpha\gamma, \alpha\delta + \beta)
          \end{equation}

          It is straightforward to see that $(1,0)$ is once again the identity element.

    \item Matrix multiplication

          The algebraic notation definition above is hard to see, but it is easier if we know that Halmos is trying to make a point with matrix and notate it as such

          \begin{equation}
              \begin{bmatrix}
                  \alpha & \beta  \\
                  \gamma & \delta
              \end{bmatrix} \boxed{\cdot} \begin{bmatrix}
                  \alpha' & \beta'  \\
                  \gamma' & \delta'
              \end{bmatrix} = \begin{bmatrix}
                  \alpha \alpha' + \beta \gamma'  & \alpha \beta' + \beta \delta'  \\
                  \gamma \alpha' + \delta \gamma' & \gamma \beta' + \delta \delta'
              \end{bmatrix}
          \end{equation}

          Through algebraic notations we could have also discovered the identity element, $\begin{bmatrix}
                  1 & 0 \\ 0 & 1
              \end{bmatrix}$

    \item Modular addition and multiplication
          As modular addition and multiplication are equivalent to normal arithmetic operations, they also share the same identity elements: the identity for addition is 0, and the identity for multiplication is 1.
\end{enumerate}

In the discussions of operations, in Problems 1-8, the notation and the language were both additive (+, sum) and multiplicative ($\times$, product). Technically there is no difference between the two, but traditionally multiplication is the more general concept. In the definition of groups, for instance (to be given soon), the notation and the language are usually multiplicative; the additive theory is included as a special case. A curious but firmly established part of the tradition is that multiplication may or may not be commutative, but addition always is. The tradition will be followed in this book, with no exceptions.

An important mini-theorem asserts that an operation can have at most one identity element. That is: if $\times$ is an operation and both $\epsilon$ and $\epsilon'$ are identity elements for it, so that

\begin{equation}
    \epsilon \times \alpha = \alpha \times \epsilon = \alpha \text{ and } \epsilon' \times \alpha = \alpha \times \epsilon' = \alpha
\end{equation}

for all $\alpha$, then

\begin{equation}
    \epsilon = \epsilon'
\end{equation}

\begin{proof}
    Use $\epsilon'$ itself for $\alpha$ in the equation involving $\epsilon$, and use $\epsilon$ for $\alpha$ in the equation involving $\epsilon'$. The conclusion is that $\epsilon \times \epsilon'$ is equal to both $\epsilon$ and $\epsilon'$, and hence that $\epsilon$ and $\epsilon'$ are equal to each other.
\end{proof}

\textit{Comment.} The proof just given is inteded to emphasize that an identity is a two-sided concept: it works from both right and left.

\section{Complex inverses}

Is there a positive integer that can be added to 3 to yield 8? Yes.

Is there a positive integer that can be added to 8 to yield 3? No.

In the well-known language of elementary arithmetic: subtraction within the domain of positive integers is sometimes possible and sometimes not.

Is there a real number that can be added to 5 to yield 0? Yes, namely -5. Every real number has a negative, and that fact guarantees that within the domain of real numbers subtraction is always possible. (To find a number that can be added to 8 to yield 3, first find a number that can be added to 3 to yield 8, and then form its negative.)

The third basic property of operations that will be needed in what follows (in addition to associativity and the existence of neutral elements) is the possibility of inversion. Suppose that $\ast$ is an operation (a temporary impartial symbol whose role in application could be played by either addition or multiplication), and suppose that the domain of $\ast$ contains a neutral element $\epsilon$, so that $\epsilon \ast \alpha = \alpha \ast \epsilon = \alpha$ for all x (\textit{Probably a typo, should be $\alpha$}). Under these circumstances an element $\beta$ is called an \textbf{inverse} of x (\textit{Likewise, should be $\alpha$}) ($\ast$ inverse) if

\begin{equation}
    \alpha \ast \beta = \beta \ast \alpha = \epsilon
\end{equation}

Obvious example: every real number $\alpha$ has a + inverse, namely $-\alpha$. Worrisome example: not every real number as a $\times$ inverse. The exception is 0; there is no real number $\beta$ such that $0 \times \beta = 1$. That is the only exception: if $\alpha \neq 0$, then the reciprocal $\alpha^{-1} (= \frac{1}{\alpha})$ is a $\times$ inverse. These examples are typical. The use of additive notation is usually intended to suggest the existence of inverses ( + inverses, negatives) for every element, whereas for multiplicatively written operations some elements can fail to be \textbf{invertible}, that is, can fail to possess inverses ( $\times$ inverses, reciprocals.)

The definition of $\ast$ inverse makes sense in complete generality, but it is useful only in case $\ast$ is associative. The point is that for associative operations an important mini-theorem holds: an element can have at most one inverse. That is: if both $\beta$ and $\gamma$ are $\ast$ inverses of $\alpha$, so that:

\begin{equation}
    \alpha \ast \beta = \beta \ast \alpha = \alpha \text{ and } \alpha \ast \gamma = \gamma \ast \alpha = \alpha
\end{equation}

then $\beta = \epsilon$.

\begin{proof}
    Combine all three $\gamma$, and $\alpha$, and $\beta$, in that order, and use the associative law. Looked at one way the answer is

    \begin{equation}
        \gamma \ast (\alpha \ast \beta) = \gamma \ast \epsilon = \gamma
    \end{equation}

    whereas the other way it is

    \begin{equation}
        (\gamma \ast \alpha) \ast \beta = \epsilon \ast \beta = \beta
    \end{equation}

    The conclusion is that the triple combination $\gamma \ast \alpha \ast \beta$ is equal to both $\gamma$ and $\beta$, and hence that $\gamma$ and $\beta$ are equal to each other.
\end{proof}

\begin{problem}
For complex multiplication (defined in Problem 4), which ordered pairs $(\alpha, \beta)$ are invertible? Is there an explicit formula for the inverses of the ones that are?
\end{problem}

\textit{Comment.} We have found previously that complex multiplication has an identity element. Suppose that an inverse exists for the ordered pair $(\alpha, \beta)$ and denote it $(\gamma, \delta)$. We then have

\begin{align}
    (\alpha, \beta) \times (\gamma, \delta) & = (\alpha \gamma - \beta \delta, \alpha \delta + \beta \gamma) = (1,0) \\
    (\gamma, \delta) \times (\alpha, \beta) & = (\gamma\alpha - \delta \beta, \gamma \beta + \delta \alpha) = (1,0)
\end{align}

thus

\begin{align}
    \alpha\gamma - \beta\delta & =  1            \\
    \alpha\delta               & = - \beta\gamma
\end{align}

We examine the case where $\beta = 0$. Then either $\alpha$ or $\delta = 0$. Suppose $\alpha = 0$, then there exists no $\delta$ that can satisfy the first equation. We then arrive at the same observation above, which is that 0 has no inverse in $\mathbf{R}$, and likewise there is no inverse for (0,0) in $\mathbf{C}$. For the case of $\alpha \neq 0$, this is simply the real set, and thus $\delta = \frac{1}{\alpha}$ satisfies.

We examine the actual interesting case where $\alpha \neq 0$ and $\beta \neq 0$. We can then derive $\delta = \frac{-\beta\gamma}{\alpha}$. Substituting into the first equation, we get

\begin{align}
    \gamma & = \frac{1}{\alpha^2 + \beta^2}              \\
    \delta & = \frac{-\beta}{\alpha(\alpha^2 + \beta^2)}
\end{align}

This is no surprise, as it matches with our understanding of the complex inverse, $\frac{\alpha - \beta i}{\alpha^2 + \beta^2}$.

\section{Affine inverses}

\begin{problem}
For the multiplication of affine transformations (defined in Problem 5), which ordered pairs $(\alpha, \beta)$ are invertible? Is there an explicit formula for the inverses of the ones that are?
\end{problem}

\textit{Comment.} We proceed likewise, suppose there exists an inverse element $(\gamma, \delta)$ for any paired variables $(\alpha, \delta)$, satisfying

\begin{equation}
    (\alpha, \beta) \boxed{\cdot} (\gamma, \delta) = (\alpha\gamma, \alpha\delta + \beta) = (1,0)
\end{equation}

We note that if $\alpha = 0$, then $\beta = 0$ makes the system consistent, but it results in any number of $\gamma$ and $\delta$, and so infinitely many $\gamma, \delta$ is possible. So there is no inverse for $(0,0)$.

Assuming then $\alpha \neq 0$, we have

\begin{align}
    \delta & = \frac{-\beta}{\alpha} \\
    \gamma & = \frac{1}{\alpha}
\end{align}

Thus the inverse is $(\frac{1}{\alpha}, \frac{-\beta}{\alpha})$.

\section{Matrix inverses}

\begin{problem}
Which of the $2 \times 2$ matrices

\[
    \begin{pmatrix}
        \alpha & \beta  \\
        \gamma & \delta
    \end{pmatrix}
\]

(for which multiplication was defined in Problem 6) are invertible? Is there an explicit formula for the inverses of the ones that are?
\end{problem}

\textit{Comment.} We likewise proceed by supposing an inverse $\begin{pmatrix}
        \alpha' & \beta'  \\
        \gamma' & \delta'
    \end{pmatrix}$

\begin{equation}
    \begin{pmatrix}
        \alpha & \beta  \\
        \gamma & \delta
    \end{pmatrix} \times
    \begin{pmatrix}
        \alpha' & \beta'  \\
        \gamma' & \delta'
    \end{pmatrix} =
    \begin{pmatrix}
        \alpha\alpha' + \beta\gamma'  & \alpha\beta' + \beta\delta'  \\
        \gamma\alpha' + \delta\gamma' & \gamma\beta' + \delta\delta'
    \end{pmatrix} =
    \begin{pmatrix}
        1 & 0 \\
        0 & 1
    \end{pmatrix}
\end{equation}

so

\begin{align}
    \alpha\beta' + \beta\delta'   & = 0 \\
    \gamma\alpha' + \delta\gamma' & = 0
\end{align}

From the second equation we have $\gamma' = \frac{-\gamma\alpha'}{\delta}$, from which we get

\begin{align}
    \alpha\alpha' + \beta\gamma'                        & = \alpha\alpha' + \beta (\frac{-\gamma\alpha'}{\delta}) \\
    \alpha' (\frac{\alpha\delta - \beta\gamma}{\delta}) & = 1                                                     \\
    \alpha'                                             & = \frac{\delta}{\alpha\delta - \beta\gamma}             \\
    \gamma'                                             & = \frac{-\gamma}{\alpha\delta - \beta\gamma}
\end{align}

We proceed likewise for $\delta' = \frac{-\alpha\beta'}{\beta}$, from which we get

\begin{align}
    \gamma\beta' + \delta\delta'                      & = \gamma\beta' + \delta (\frac{-\alpha\beta'}{\beta}) \\
    \beta' (\frac{\beta\gamma - \delta\alpha}{\beta}) & = 1                                                   \\
    \beta'                                            & = \frac{\beta}{\beta\gamma - \delta\alpha}            \\
    \delta'                                           & = \frac{-\alpha}{\beta\gamma - \delta\alpha}
\end{align}

This establishes the formula we are looking for. We also note the appearance of the \textbf{determinant}, $\alpha\delta - \beta\gamma$.

\section{Abelian groups}

Numbers can be added, subtracted, multiplied, and (with one infamous exception) divided. Linear algebra is about concepts called scalars and vectors. Scalars are usually numbers; to understand linear algebra it is necessary first of all to understand numbers, and, in particular, it is necessary to understand what it means to add and subtract them. The general concept that lies at the heart of such an understanding is that of \textbf{abelian group}.

Consider, as an example, the set $\Z$ of all integers (positive, negative, or zero) together with the operation of addition. The sum of two integers is an integer such that

\quad addition is \textbf{commutative}, meaning that the sum of two integers is independent of the order in which they are added,

\begin{equation}
    x + y = y + x;
\end{equation}

\quad addition is \textbf{associative}, meaning that the sum of three integers, presented in a fixed order, is independent of the order in which the two additions between them are performed,

\begin{equation}
    (x + y) + z = x + (y + z);
\end{equation}

\quad the integer \textbf{0} plays a special role in that it does not change any integer that it is added to,

\begin{equation}
    x + 0 = 0 + x = x;
\end{equation}

\quad and every addition can be "undone" by another one, namely the addition of the \textbf{negative} of what was just added,

\begin{equation}
    x + (-x) = (-x) + x = 0
\end{equation}

This example is typical. The statements just made about $\Z$ and + are in effect the definition of the concept of abelian group. Almost exactly the same statements can be made about every abelian group; the only differences are terminological (the words "integer" and "addition" may be replaced by others) and notational (the symbols 0 and + may be replaced by others).

Another example is the set $\Z_{12}$ consisting of the integers between 0 and 11 inclusive, with an operation of addition (temporarily denoted by $\ast$) defined this way: if the sum of two elements of $\Z_{12}$ (in the ordinary meaning of sum) is less than 12, then the new sum is equal to that ordinary sum.

\begin{equation}
    x \ast y = x + y
\end{equation}

but if their ordinary sum is 12 or more, then the new sum is the ordinary with 12 subtracted,

\begin{equation}
    x \ast y = x + y = 12.
\end{equation}

The operation $\ast$ is called addition \textbf{modulo 12}, and is usually denoted by just plain +, or, if desired, by + followed soon by an explanatory "modulo 12". The verification that the four typical sentences stated above for $\Z$ are true for $\Z_{12}$ is a small nuisance, but it is painless and leads to no surprises. (The closest it comes to a surprise is that the role of the negative of $x$ is played by $12-x$.)

Here is another example of an abelian group: the set $\R$, of \textbf{positive} real numbers, with an operation, temporarily denoted by $\ast$, defined as ordinary numerical \textbf{multiplication}:

\begin{equation}
    x \ast y = xy.
\end{equation}

Everybody believes commutativity and associativity; the role of zero is played this time by the real number 1,

\begin{equation}
    x \ast 1 = 1 \ast x = x,
\end{equation}

and the role of the negative of $x$ is played by the reciprocal of $x$

\begin{equation}
    x \ast \frac{1}{x} = \frac{1}{x} \ast x = 1
\end{equation}

The general definition of an abeligan group should be obvious by now: it is a set $\mathbb{G}$ with an operation of "addition" defined in it (so that whenever $x$ and $y$ are in $\mathbb{G}$, then $x+y$ is again an element of $\mathbb{G}$), satisfying the four conditions discussed above. (They are: commutativity; the existence of a \textit{zero}; and, corresponding to each element, the existence of a negative of that element.)

The word "abelian" means exactly the same as "commutative". If an operation in a set satisfies the last three conditions but not necessarily the first, then it is called a \textbf{group}. Non-commutative groups also enter the study of linear algebra, but not till later, and not as basically as the commutative ones.

\begin{problem}
(a) If a new operation $\ast$ is defined in the set $\R_+$ of positive real numbers by

\begin{equation}
    x \ast y = \min (x,y)
\end{equation}

does $\R_+$ become an abelian group?

(b) If an operation $\ast$ is defined in the set $\{1,2,3,4,5\}$ of positive integers by

\begin{equation}
    x \ast y = \max(x,y)
\end{equation}

does that set become an abelian group?

(c) If $x$ and $y$ are elements of an abelian group such that $x+y = y$, does it follow that $x=0$?
\end{problem}

\begin{minipage}{300px}
    \textit{Working.} (a) For a set of three numbers $(x,y,z)$, there will always exist one smallest number, which will be the result of the $x \ast y \ast z$ operations.

    The operation is also associative

    \begin{equation}
        x \ast y = \min (x,y) = \min(y,x) = y \ast x
    \end{equation}

    It does not however have an identity element, since this would need to be a number which is larger than every other number. And likewise it does not have an inverse element, at least one that can be obtained by a function: for a given number $y$, there are infinitely many elements $x$ such that $min(x,y) = y$. Hence the $min$ operation does not form an abelian group.

    (b) This is likewise an operation that is commutative and associative, but there does not exist an identity element and an inverse element. So this does not form an abelian group.

    (c) As the set is an abelian group, there must exist an element $-y$ that is the inverse of $y$. Consider the operation

    \begin{align}
        (x + y) + (-y) & = y + (-y) = 0 \\
        x + (y + (-y)) & = x + 0 = 0
    \end{align}

    By the associative property of abelian group, $x = 0$.
\end{minipage}

\textit{Comment.} The abbreviation "min" and "max" are for minimum and maximum; $min(2,3) = 2$, $max(-2,-3) = -2$, and $min(5,5) = 5$.

Parts (a) and (b) of the problem test the understanding of the definition of abelian group. The beginning of a systematic development of group theory (abelian or not) is usually a sequence of axiom splitting delicacies, which are fussy but can be fun. A sample is the mini-theorem discussed in Problem 9, the one that says that there can never be more than one element that acts the way $0$ does. Part (c) of this problem is another sample of the same kind of thing. It is easy, but it's here because it's useful and, incidentally, because it shows how the defining axioms of groups can be useful. What was proved in Problem 9 is that if an element acts the way $0$ does for \textit{every} element, then it must be 0; part (c) here asks about elements that act the way 0 does for only \text{one} element.

\section{Groups}

According to the definition in Problem 13 a set endowed with an operation that has all the defining properties of an abelian group except possibly the first, namely commutativity, is called just simply a \textbf{group}. (Recall that the word "abelian" is a synonym for "commutative".) Emphasis: the operation is an essential part of the definition; if two different operations on the same set both satisfy the defining conditions, the results are regarded as two different groups.

Probably the most familiar example of an abelian group is the set $\Z$ of all integers (positive, negative, and zero), or, better said, the group is the pair $\{\Z, + \}$, the set $\Z$ together with, endowed with, the operation of addition. It is sometimes possible to throw away some of the integers and still have a group left; thus, for instance, the set of all even integers is a group. Throwing things away can, however, be dangerous; the set of positive integers is not an additive group (there is no identity element: 0 is missing), and neither is the set of non-negative integers (once 0 is put back in it makes sense to demand inverses, but the demand can be fulfilled only by putting all the negaitve integers back in too).

The set of real numbers with addition, in symbols $\{\R, +\}$, is a group, but $\{\R, \times\}$ is not - the number 0 has no inverse. The set of non-zero real numbers on the other hand, is a multiplicative group. The same comments apply to the set $\C$ of complex numbers. The set of positive real numbers is a group with respect to multiplication, but the set of negative real numbers is not - the product of two of them is not negative.

Group theory is deep and pervasive - no part of mathematics is free of its influence. At the beginning of linear algebra not much of it is needed, but even here it is a big help to be able to recognize a group when one enters the room.

\begin{problem}
Is the set of all affine transformation $\xi \mapsto \alpha\xi + \beta$ (with the operation of functional composition) a group? What about the set of all $2 \times 2$ matrices

\[
    \begin{pmatrix}
        \alpha & \beta  \\
        \gamma & \delta
    \end{pmatrix}
\]

(with matrix multiplication)? Is the set of non-zero integers modulo 6 (that is: the set of numbers $1,2,3,4,5$) a group with respect to multiplication modulo 6? What if 6 is replaced by 7; does the conclusion remain true or does it change?
\end{problem}

\textit{Comment.} The symbol $\mapsto$, called the \textbf{barred arrow}, is commonly used for functions; it serves the purpose of indicating the "variable" that a function depends on. To speak of the function "the function $2x+3$" is bad form; what the expression $2x+3$ denotes is not a function but the value of a function at the number $x$. Correct language speaks of the function

\begin{equation}
    x \mapsto 2x + 3
\end{equation}

which is an abbreviation for "the function whose value at each $x$ is $2x+3$".

\textit{Working.}

Affine transformation with the operation of functional composition may not form a group if the affine transformation has no inverse.

Let $f$ denote $x \mapsto \alpha x + \beta$ and $g$ denote $x \mapsto \gamma x + \delta$. The composition of $f g$ is also a function.

\begin{align}
    \alpha x + \beta & \mapsto \gamma (\alpha x + \beta) + \delta     \\
                     & \equiv \alpha \gamma x + \gamma \beta + \delta
\end{align}

We have proven before that affine transformations are associative (though not commutative). We have also proven that there is an identity element in affine transformation, $(1,0)$. Lastly, the existence of an inverse may not be guaranteed, which is the case of $(\alpha, \beta) = (0, \beta)$, where the transformation "squished" the vector and so there is no inverse element.

This is likewise the case for matrix multiplication. The $2\times2$ case has been proven above to be associative (but not commutative), to have an identity element, but not always have an inverse element.

The situation with the set of modulo 6 is more interesting. The table below lets us examine all the possibility

\begin{center}
    \begin{tabular}{l|ccccc}
        $\times$ & 1 & 2 & 3 & 4 & 5 \\
        \hline
        1        & 1 & 2 & 3 & 4 & 5 \\
        2        & 2 & 4 & 0 & 2 & 4
    \end{tabular}
\end{center}

The element 0 is not part of the set $\{1,2,3,4,5\}$, so this set is not closed under multiplication. This situation will not happen for modulo 7 as no two elements will multiply up to 7; we can establish the table to see it visually.

\begin{center}
    \begin{tabular}{l|cccccc}
        $\times$ & 1 & 2 & 3 & 4 & 5 & 6 \\
        \hline
        1        & 1 & 2 & 3 & 4 & 5 & 6 \\
        2        &   & 4 & 6 & 1 & 3 & 5 \\
        3        &   &   & 2 & 5 & 1 & 4 \\
        4        &   &   &   & 2 & 6 & 3 \\
        5        &   &   &   &   & 4 & 2 \\
        6        &   &   &   &   &   & 1
    \end{tabular}
\end{center}

The set is closed under multiplication. It is also associative (and commutative in this case). There exists an identity element, namely 1. Lastly, the existence of an inverse element. From the table we can see that each element maps to a full set of $\{1,2,3,4,5,6\}$, so there always exists an inverse element. On a more number theoretical take, this is called the modulo inverse, and it exists when the coefficient and the modulo exponent are coprime. So while the set of modulo 6 does not always have an inverse element, the set of modulo 7 always does, and so the modulo set of 7 is a group while the modulo 6 is not.

\section{Independent group axioms}

A group is a set with an operation that has three good properties, namely associativity, the existence of an identity element, and the existence of inverses. Are those properties totally independent of one another, or is it the case that some of them imply some of the others? So, for example, must an associative operation always have an identity element? The answer is no, and that negative answer is one of the first things that most children learn about arithmetic. We all learn early in life that we can add two positive integers and get a third one, and we quickly recognize that that third one is definitely different from both of the numbers that we started with - the discovery of zero came to humanity long after the discovery of addition, and it comes similarly late to each one of us. Very well then, if we have an associative operation that does possess an identity element, does it follow that every element has an inverse? The negative answer to that question reaches most of us not long after our first arithmetic disappointment (see Problem 13): in the set $\{0,1,2,3,...\}$ we can add just fine, and $0$ is an identity element for addition, but $+$ inverses are hard to come by - subtraction cannot always be done. After these superficial comments there is really only one sensible question left to ask.

\begin{problem}
Can there exist a non-associative operation with an identity element, such that every element has an inverse?
\end{problem}

\textit{Working.} Yes, and the "natural" answer is the exponentiation operation on $\R_+$. The operation is non-associative ($\alpha^{\beta^\gamma}$ is not the same as $\alpha^{\beta\gamma}$); there is an identity element, 1. The proof that an inverse element exists however is more difficult, but the operation is well defined for $\frac{1}{\beta}$.

\begin{equation}
    \alpha \exp \beta \exp \frac{1}{\beta} = \alpha
\end{equation}

\section{Fields}

If, temporarily, "number" is interpreted to mean "integer", then numbers can be added and subtracted and multiplied, but, except accidentally as it were, they cannot be divided. If we insist on dividing them anyway, we leave the domain of integers and get the set $\Q$ of all quotients of integers - in other words the set $\Q$ of rational numbers, which is a "field".

(Does anyone know about rational numbers? A real number is called \textbf{rational} if it is the ratio of two integers. In other words, $x$ is rational just in case there exists integers $m$ and $n$ such that $x = \frac{m}{n}$. Examples: $\frac{2}{3}$, $-\frac{2}{3}$, 0,1,10, -10, $5^5$. Note: $\frac{4}{6}$ and $\frac{-12}{-18}$ are additional representations of the rational number $\frac{2}{3}$ already mentioned; there are many others. Celebrated counterexamples: $\sqrt{2}$ and $\pi$. A proof that $\sqrt{2}$ is not rational was known to humanity well over 2000 years ago; the news about $\pi$ is only a little more than 200 years old. That $(\Q, +)$ is a group needs to be checked, of course, but the check is easy, and the same is true for $(\Q - \{0\}, \times)$.)

Probably the best known example of a field is the $\R$ of real numbers endowed with the operations of addition and multiplication. As far as addition goes, $\R$ is an abelian group, and so is $\Q$. The corresponding statement for multiplication is not true; zero causes trouble. Since $0x = 0$ for every real number $x$, it follows that there is no $x$ such that $0x = 1$; the number 0 does not have a multiplicative inverse. If, however, $\R$ is replaced by the set $\R^\ast$ of real numbers different from 0 (and similarly $\Q$ is replaced by the set $\Q^\ast$ of rational numbers different from 0), then everything is all right again: $\R^\ast$ with multiplication is an abelian group, and so is $\Q^\ast$.

It surely does not come as a surprise that the same statements are true about the set $\C$ of all complex numbers with addition and multiplication, and, indeed, $\C$ is another example of a field. The properties of $\Q$ and $\R$ and $\C$ that have been mentioned so far, are, however, not quite enough to guess the definition of a field from. What the examples suggest is that a field has two operations; what they leave out is a connection between them. It is mathematical malpractice to endow a set with two different structures that have nothing to do with each other. In the examples already mentioned addition and multiplication together form a pleasant and useful conspiracy (in fact two conspiracies), call the \textbf{distributive law} (or laws):

\begin{equation}
    \alpha(x+y) = \alpha x + \alpha y \text{ and } (\alpha + \beta) x = \alpha x + \beta x
\end{equation}

and once that is observed the correct definition of fields become guessable. A \textbf{field} is a set $\F$ with two operations $+$ and $\times$ such that with $+$ the entire set $\F$ is an abelian group, with $\times$ the diminished set $\F^\ast$ (omit 0) is an abelian group, and such that the distributive laws are true.

(Is it clear what "0" means here? It is intended to mean the identity element of the additive group $\F$. The notational conventions for real numbers are accepted for all fields: 0 is always the additive neutral element, 1 is the multiplicative one, and, except at rare times of emphasis, multiplication is indicated simply by juxtaposition.)

Some examples of fields, les obvious than $\R$, $\Q$, and $\C$, deserve mention. One good example is called $\Q(\sqrt{2})$; it consists of all numbers of the form $\alpha + \beta \sqrt{2}$ where $\alpha$ and $\beta$ are rational; the operations are the usual addition and multiplication of real numbers. All parts of the definition, except perhaps one, are obvious. What may not be obvious is that every non-zero element of $\Q(\sqrt{2})$ has a reciprocal. The proof that it is true anyway, obvious or no, is the process of rationalizing the denominator (used before in Solution 10). That is: to determine $\frac{1}{\alpha + \beta \sqrt{2}}$, multiply both numerator and denominator by $\alpha - \beta \sqrt{2}$ and get

\begin{equation}
    \frac{\alpha-\beta \sqrt{2}}{\alpha^2 - 2\beta^2} = \frac{\alpha}{\alpha^2 - 2\beta^2} - \frac{\beta}{\alpha^2 - 2\beta^2}\sqrt{2}
\end{equation}

The only thing could possibly go wrong with this procedure is that the denominator $\alpha^2 - 2\beta^2$ is zero, and that cannot happen (unless both $\alpha$ and $\beta$ are zero to begin with) - the reason it cannot happen is $\sqrt{2}$ is not rational.

Could it happen in a field that the additive identity element is equal to the multiplicative one, that is that $0 = 1$? That is surely not the intention of the definition. A legalistic loophole that avoids such degeneracy is to recall that a group is never the empty set (because, by assumption, it contains an identity element). It follwos that if $\F$ is a field, then $\F - \{0\}$ is not empty. That does it. 1 is an element of $\F - \{0\}$, and therefore $1 \neq 0$.

If $\F$ is a field, then both $\F$ with $+$ and $\F^\ast$ with $\times$ are abelian groups, but neither of these facts has anything to do with the \textit{multiplicative} properties of the \textit{additive} inverse 0. As far as they are concerned, do fields behave the way $\Q$, $\R$, and $\C$ do, or does the generality permit some unusual behavior?

\begin{problem}
Must multiplication in a field be commutative?
\end{problem}